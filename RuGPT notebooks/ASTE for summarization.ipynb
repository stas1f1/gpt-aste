{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a817d914-bbc9-4dca-a690-9e7f580533f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from textwrap import wrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e635f560-aef9-40db-91b8-369ab0a668eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/asurikov\n"
     ]
    }
   ],
   "source": [
    "cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "873e75fd-b309-429c-8439-bf3d53d2f000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPE-NEWS  ASTE  README.ipynb  cherniak  tmp\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9290062-ba58-453f-9e78-8fa293739803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1536)\n",
       "    (wpe): Embedding(2048, 1536)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): Block(\n",
       "        (ln_1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"ASTE/rugpt3large_based_on_gpt2_finetuned-50x3\"\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a062216-9cd3-4a68-8a60-374bf5b53eaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5694a18-5599-4803-bbc1-9c39368cbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_pot = \"ASTE/examples/pot.csv\"\n",
    "data_path_styler = \"ASTE/examples/styler.csv\"\n",
    "data_path_buds = \"ASTE/examples/buds.csv\"\n",
    "data_path_soundbar = \"ASTE/examples/soundbar.csv\"\n",
    "\n",
    "res_path_pot = \"ASTE/examples/evals/pot_eval\"\n",
    "res_path_styler = \"ASTE/examples/evals/styler_eval\"\n",
    "res_path_buds = \"ASTE/examples/evals/buds_eval\"\n",
    "res_path_soundbar = \"ASTE/examples/evals/soundbar_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "798c1d91-cd42-4e19-99aa-f4131df55047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Понравился дизайн- люблю чайники с прозрачным ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Красивый, с синей подсветкой, не громоздкий, ш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Очень эффектный чайничек. На подставке не заст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Плохой пластик. Как уже писали здесь, запах хи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Думала, будет лучше. Зачеркнутая цена в 3000 э...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Понравился дизайн- люблю чайники с прозрачным ...\n",
       "1  Красивый, с синей подсветкой, не громоздкий, ш...\n",
       "2  Очень эффектный чайничек. На подставке не заст...\n",
       "3  Плохой пластик. Как уже писали здесь, запах хи...\n",
       "4  Думала, будет лучше. Зачеркнутая цена в 3000 э..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pot = pd.read_csv(data_path_pot)\n",
    "print(len(data_pot))\n",
    "data_pot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4945374d-e5dd-4083-9503-1dd81c516740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Оригинал, все проверила полностью. В составе 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ну я очень хотела такой стайлер и наконец муж ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Это популярный стайлер, оригинал однозначно и ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Только что получила. Как же я боялась подделки...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>У меня уже есть фен, покупала в фирменном мага...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Оригинал, все проверила полностью. В составе 8...\n",
       "1  Ну я очень хотела такой стайлер и наконец муж ...\n",
       "2  Это популярный стайлер, оригинал однозначно и ...\n",
       "3  Только что получила. Как же я боялась подделки...\n",
       "4  У меня уже есть фен, покупала в фирменном мага..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_styler = pd.read_csv(data_path_styler)\n",
    "print(len(data_styler))\n",
    "data_styler.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bd84ba5-45ab-4442-9e16-9f0982cb53fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_buds = pd.read_csv(data_path_buds)\n",
    "data_soundbar = pd.read_csv(data_path_soundbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cbfee9b-bc05-4f10-a52d-9bc1558eaf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(text):\n",
    "  return 'Текст: ' + \" \".join(text.strip().split('\\n')) + '\\n'\n",
    "\n",
    "texts_pot =    list(map(format_input, data_pot[\"text\"]))\n",
    "texts_styler = list(map(format_input, data_styler[\"text\"]))\n",
    "texts_buds =   list(map(format_input, data_buds[\"Текст\"]))\n",
    "texts_soundbar =   list(map(format_input, data_soundbar[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e2a50-3cbe-4d8b-b6ab-02f05c714eb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c5e11f0-3e08-49ce-ac24-64fbe3231aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Текст: Позже были Huawei Freebuds Pro. Звук отличный, жесты есть. Но не сидели в ушах — напрягали, после чего уши болели. Плюс к этому у них был просто ужасный микрофон. Когда говорил на умеренно шумной улицы, собеседник слышал меня как из подушки или не слышал вообще.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "05265849-c8e5-4868-82f8-4d91e4110a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "звук: отличный: положительно \n",
      "жесты: есть: положительно \n",
      "микрофон: ужасный: негативно \n",
      "микро: не сидели в ушах: негативно \n",
      "жизнь: не напрягали уши: положительно\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "input_ids = input_ids.cuda()\n",
    "out = model.generate(\n",
    "        input_ids, \n",
    "        min_length=100, \n",
    "        max_length=300, \n",
    "        eos_token_id=5, \n",
    "        pad_token_id=1,\n",
    "        top_k=1,\n",
    "        top_p=0.0,\n",
    "        no_repeat_ngram_size=5\n",
    ")\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "#print('\\n' + '\\n'.join(wrap(prompt, 100)) + '\\n')\n",
    "print(generated_text[len(prompt):].split(\"\\n\\n\")[0])\n",
    "#print(generated_text[len(prompt):].split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b1a2dd-9285-4f3a-a35b-2d831372e081",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "832a19b6-b6dc-4fe1-a6a0-6ebf04be4836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset_eval_aste_joint(texts):\n",
    "    results = []\n",
    "    for index in tqdm(range(len(texts))):\n",
    "        prompt = texts[index]\n",
    "\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        input_ids = input_ids.cuda()\n",
    "        out = model.generate(\n",
    "                input_ids, \n",
    "                min_length=100, \n",
    "                max_length=350, \n",
    "                eos_token_id=5, \n",
    "                pad_token_id=1,\n",
    "                top_k=2,\n",
    "                top_p=0.0,\n",
    "                no_repeat_ngram_size=5\n",
    "        )\n",
    "        generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "        results.append(generated_text[len(prompt)-1:].strip().split(\"\\n\\n\")[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c26be59-c98d-449a-844b-1be708bdc8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [09:39<00:00, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Понравился дизайн- люблю чайники с прозрачным стеклом, красивая внутренняя подсветка кипящей воды , отлично,что без воды не работает- защита от включения без воды- безопасно. Хороший обьем- сразу на компанию 1,8 литров! Отличный чайник за небольшие деньги.Приехал сразу в новогодней упаковке- подарю на Новый Год!\n",
      "\n",
      "Триплеты: \n",
      "дизайн: нравится: положительно \n",
      "подсветка: красивая: положительно \n",
      "объем: хороший: положительно \n",
      "чайник: отличный: положительно \n",
      "подарочная упаковка: в новой: положительно\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pot_results = run_dataset_eval_aste_joint(texts_pot)\n",
    "print(texts_pot[0])\n",
    "print(\"Триплеты: \\n\" + pot_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e9ff96d-5682-47fc-95bd-d09fa57b1355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [06:39<00:00, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Оригинал, все проверила полностью. В составе 8 насадок, подойдет для длинных и коротких волос. Волосы ниже талии, мне кажется, лучше взять лонг. Есть 3 скорости обдува и 3 режима подачи воздуха. Соответственно, самый горячий воздух это третий режим - очень хорошо закручивает локоны, укладка сохраняется надолго. Самый холодный воздух - он действительно дует прохладным, но с ним конечно локоны так уже не закручиваются. Есть щетка для чистки фильтра. ее все игнорируют, а зря. Надо чистить этой щеточкой стайлер, тогда он прослужит долгие годы, потому что не сказать, что у нас чистый воздух, у меня забивается пылью довольно быстро, раз в месяц чищу на всякий случай. У этой модели еще идет сумка, с ней я езжу в отпуск, очень удобно.\n",
      "\n",
      "Триплеты: \n",
      "насадка: для длинных и коротких: положительно \n",
      "обдув: 3 режима: положительно \n",
      "режим подачи воздуха: 3 скорости: положительно \n",
      "щетка: идет в комплекте: положительно \n",
      "сумка: очень удобная: положительно \n",
      "стайлер: служит очень долго: положительно\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "styler_results = run_dataset_eval_aste_joint(texts_styler)\n",
    "print(texts_styler[0])\n",
    "print(\"Триплеты: \\n\" + styler_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6892af76-5b39-419e-88f8-30f601e89d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [20:46<00:00, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Маленький и удобный кейс и наушники. Удобное приложение со всеми необходимыми настройками и без излишеств. Звук более чем хороший! Приятные басы, звук не резкий. Режим \"Звуковой фон\" сделан будто для шпионов, которые чужие разговоры любят подслушивать. Что ж, буду пробовать :) Шумодав достойный! Качество сборки. Кейс не скрипит и открывается с небольшим усилием. Функция нахождения наушников — Самсунг, спасибо вам за это. Сидел в забегаловке и неосторожно вытащил наушники, не успев положить левый в кейс. Он выпал из рук и буквально укатился куда-то под большие диваны. Благо есть возможность \"прозвонить\". Прислушался — нашёл!\n",
      "\n",
      "Триплеты: \n",
      "кейс: удобный, удобный: положительно \n",
      "входные устройства: удобные, удобные: положительно \n",
      "звук: более чем хороший: положительно \n",
      "функция нахождения наушников: Самсунг: положительно \n",
      "качество сборки: отличное: положительно \n",
      "шумодав: достойный: положительно\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "buds_results = run_dataset_eval_aste_joint(texts_buds)\n",
    "print(texts_buds[0])\n",
    "print(\"Триплеты: \\n\" + buds_results[0])\n",
    "with open(res_path_buds, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(buds_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4269804a-d82a-4a35-84c3-86aa7c12d1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [21:43<00:00, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: Во-первых, надо трезво понимать, что за 3200 рублей купить многополосную акустику с супер детализацией и басами невозможно. Но сказать, что басов нет или детали совсем жёванные — будет неправдой. Во-вторых, брал этот саундбар для довольно старенького телевизора Haier, у которого родной звук, как из унитаза. После подключения саундбара разница была небо и земля. В третьих, и последнее. Самый главный аргумент — это бюджетность, поэтому буду честным, и оценку ставлю за соотношение цена-качество. + басы есть, + громкости хватает с лихвой, + присутствует голосовое меню, сообщающее о режиме подключения, + лампочка во время работы не горит (не отвлекает), + боковой джойстик регулировки стильный, как и сам саундбар.\n",
      "\n",
      "Триплеты: \n",
      "басы: есть: положительно \n",
      "детализация: супер: положительно \n",
      "басы: есть: положительно\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soundbar_results = run_dataset_eval_aste_joint(texts_soundbar)\n",
    "print(texts_soundbar[0])\n",
    "print(\"Триплеты: \\n\" + soundbar_results[0])\n",
    "with open(res_path_soundbar, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(soundbar_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "508c2cc8-6e61-4427-9dae-4ac1bcd9e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(res_path_pot, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(pot_results))\n",
    "with open(res_path_styler, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(styler_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b2c5b-fa91-4cb9-8cee-1789b4350c04",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Methods for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8a5152-5599-4e5f-b24e-44327c7cf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomize_triplet(aspect, opinions, polarity):\n",
    "  opinions = opinions.split(', ')\n",
    "  return [[aspect, opinion, polarity] for opinion in opinions] \n",
    "\n",
    "def convert_polarity(span):\n",
    "  if span == 'отрицательно' or span == 'негативно':\n",
    "    return -1\n",
    "  if span == 'нейтрально':\n",
    "    return 0\n",
    "  if span == 'положительно' or span == 'позитивно':\n",
    "    return 1\n",
    "  else:\n",
    "    #error-code\n",
    "    return -2\n",
    "\n",
    "#loading in results\n",
    "def load_ATE_results(path):\n",
    "  results = \"\"\n",
    "  with open(path, \"r\") as file:\n",
    "    results = file.read()\n",
    "  return [line.strip().split(', ') for line in results]\n",
    "\n",
    "def load_ASTE_results(path):\n",
    "  results = \"\"\n",
    "  with open(path, \"r\") as file:\n",
    "    results = file.read()\n",
    "  \n",
    "  #wrong_format\n",
    "  wf = 0\n",
    "  #right_format\n",
    "  rf = 0\n",
    "  #wrong_polarity - not in any class\n",
    "  wp = 0\n",
    "  #right_polarity\n",
    "  rp = 0\n",
    "  #wrong_polarity_format - polarity contains junk\n",
    "  wpf = 0\n",
    "  #right_polarity_format\n",
    "  rpf = 0\n",
    "\n",
    "  res_triplets = []\n",
    "\n",
    "  for group in results.split('\\n\\n'):\n",
    "    group_triplets = []\n",
    "    for triplet in group.split('\\n'):\n",
    "      spans = [span.strip() for span in triplet.split(': ')]\n",
    "      if len(spans) != 3:\n",
    "        wf += 1\n",
    "      else:\n",
    "        rf += 1\n",
    "        \n",
    "        #we check polarity span - does it contain junk?\n",
    "        polarity_span = spans[2].split()\n",
    "        if (len(polarity_span) > 1):\n",
    "          wpf += 1\n",
    "          spans[2] = polarity_span[0]\n",
    "        else:\n",
    "          rpf += 1\n",
    "        \n",
    "        #we check polarity class\n",
    "        polarity_converted = convert_polarity(spans[2])\n",
    "        if polarity_converted == -2:\n",
    "          wp += 1\n",
    "        else:\n",
    "          rp += 1\n",
    "\n",
    "        group_triplets.extend(atomize_triplet(*spans))\n",
    "    res_triplets.append(group_triplets)\n",
    "\n",
    "  f_rate, p_rate, pf_rate = 0, 0, 0\n",
    "\n",
    "  if wf + rf > 0:\n",
    "    f_rate = wf / (wf + rf)\n",
    "  if wp + rp > 0:\n",
    "    p_rate = wp / (wp + rp)\n",
    "  if wpf + rpf > 0:\n",
    "    pf_rate = wpf / (wpf + rpf)\n",
    "  \n",
    "  return res_triplets, f_rate, p_rate, pf_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f830c0-12c3-4d61-b637-6b110be10ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_triplets(tl):\n",
    "  print(\"\\n\".join([\": \".join(triplet) for triplet in tl]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81125a-3940-4726-a9db-6ed0a1a0f341",
   "metadata": {},
   "source": [
    "# Methods for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b939cf56-e806-4db7-8a2c-45236dc76444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence_transformers in ./.local/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.8/site-packages (from sentence_transformers) (3.7)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from sentence_transformers) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.8/site-packages (from sentence_transformers) (1.1.3)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.8/site-packages (from sentence_transformers) (0.1.97)\n",
      "Requirement already satisfied: tqdm in /devtools/anaconda/lib/python3.8/site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.local/lib/python3.8/site-packages (from sentence_transformers) (4.25.1)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.8/site-packages (from sentence_transformers) (1.9.3)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.8/site-packages (from sentence_transformers) (0.9.1+cu101)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.local/lib/python3.8/site-packages (from sentence_transformers) (1.8.1+cu101)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.local/lib/python3.8/site-packages (from sentence_transformers) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /devtools/anaconda/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /devtools/anaconda/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /devtools/anaconda/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /devtools/anaconda/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in /devtools/anaconda/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.8/site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: click in /devtools/anaconda/lib/python3.8/site-packages (from nltk->sentence_transformers) (8.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.8/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /devtools/anaconda/lib/python3.8/site-packages (from torchvision->sentence_transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /devtools/anaconda/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /devtools/anaconda/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /devtools/anaconda/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /devtools/anaconda/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/home/asurikov/.cache/torch/sentence_transformers/inkoziev_sbert_synonymy/ were not used when initializing BertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "st_model = SentenceTransformer('inkoziev/sbert_synonymy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20782bf-a316-412e-a014-df62120f8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method returns precision, recall, F1 from TP, FP, FN\n",
    "def prf1(TP, FP, FN):\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, F1\n",
    "\n",
    "#This method finds longest common sequence\n",
    "def LCS(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 0\n",
    "    res = max(LCS(a[:-1], b), LCS(a, b[:-1]))\n",
    "    if a[-1] == b[-1]:\n",
    "        res = max(res, LCS(a[:-1], b[:-1]) + 1)\n",
    "    return res\n",
    "\n",
    "#This method takes two lists of strings and returnsut ids of left difference, intersection and right difference\n",
    "#and a dictionary of ids in the right list that correspond to id from the left\n",
    "#SBERT (st_model) and Transformer (tokenizer) tokenizers should be constructed before use\n",
    "def get_span_ps_sets_ids(spans, p_ratio = 0.75, s_ratio = 0.5):\n",
    "    #exit if one list is less than 2\n",
    "    if len(spans)<2:\n",
    "      return [], []\n",
    "\n",
    "    indices = np.zeros(len(spans)).tolist()\n",
    "    strings = np.full((len(spans)), \"\")\n",
    "\n",
    "    for i in range(0, len(spans)-1):\n",
    "\n",
    "        #Compute embeddings for both lists\n",
    "\n",
    "        embs_curr = st_model.encode([spans[i]], convert_to_tensor=True)\n",
    "        embs_right = st_model.encode(spans[i+1:], convert_to_tensor=True)\n",
    "        tf_ids_left = tokenizer.encode(spans[i], return_tensors=\"pt\").tolist()[0]\n",
    "        tf_ids_right = [tokenizer.encode(span, return_tensors=\"pt\").tolist()[0] for span in right]\n",
    "\n",
    "        left_indices = set()\n",
    "        right_indices = set()\n",
    "        pairs = {}\n",
    "\n",
    "        #Compute cosine-similarities\n",
    "        cosine_scores = util.cos_sim(embs_left, embs_right)\n",
    "\n",
    "        for n in range(cosine_scores.size(0)):\n",
    "          for m in range(cosine_scores.size(1)):\n",
    "\n",
    "            #Compute longest common sequence\n",
    "            lcs_len = LCS(tf_ids_left[n], tf_ids_right[m])\n",
    "            lcs_ratio_left = lcs_len / len(tf_ids_left[n])\n",
    "            lcs_ratio_right = lcs_len / len(tf_ids_right[m])\n",
    "\n",
    "            #We find pairs of strings to be equal if LCS takes up more than [s_ratio] in one of tokenized\n",
    "            #representations, or if SBERT is more than [r_ratio] confident in their similarity\n",
    "            if (cosine_scores[n][m] >= p_ratio or lcs_ratio_left >= s_ratio or lcs_ratio_right >= s_ratio):\n",
    "              left_indices.add(n)\n",
    "              right_indices.add(m)\n",
    "              if n not in pairs.keys():\n",
    "                pairs[n] = [m]\n",
    "              else:\n",
    "                pairs[n].append(m)\n",
    "\n",
    "    return \n",
    "\n",
    "#Basic check - works for all kids of tasks\n",
    "def get_metrics_strict(pred, target):\n",
    "  TP, FP, FN = 0, 0, 0\n",
    "  for i in range(len(pred)):\n",
    "    TP += len([x for x in pred[i] if x in target[i]])\n",
    "    FP += len([x for x in pred[i] if x not in target[i]])\n",
    "    FN += len([x for x in target[i] if x not in pred[i]])\n",
    "  \n",
    "  return TP, FP, FN\n",
    "\n",
    "def convert_polarity(span):\n",
    "  if span == 'отрицательно' or span == 'негативно':\n",
    "    return -1\n",
    "  if span == 'нейтрально':\n",
    "    return 0\n",
    "  if span == 'положительно' or span == 'позитивно':\n",
    "    return 1\n",
    "  else:\n",
    "    #error-code\n",
    "    return -2\n",
    "\n",
    "#paraphrases & subsequenses\n",
    "def get_metrics_ATE_ps(pred, target, p_ratio = 0.75, s_ratio = 0.5):\n",
    "  TP, FP, FN = 0, 0, 0\n",
    "  for i in range(len(pred)):\n",
    "    left, intersection, right, pairs = get_span_ps_sets_ids(pred[i], target[i], p_ratio, s_ratio)\n",
    "    TP += len(intersection)\n",
    "    FP += len(left)\n",
    "    FN += len(right)\n",
    "  return TP, FP, FN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57ad3028-170d-44b7-9744-37794aab7ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'return_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mst_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi darling\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLOL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'return_dict'"
     ]
    }
   ],
   "source": [
    "st_model.encode([\"Hi darling\", \"LOL\"], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa6e53b-ae8f-4262-ad5a-a597793d28f4",
   "metadata": {},
   "source": [
    "# Load and analyze evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1450c368-96a8-44bf-8782-1be63b4cf101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: rugpt3large_based_on_gpt2_finetuned-50x3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format error</th>\n",
       "      <th>polarity error-class</th>\n",
       "      <th>polarity format error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SMM - pot reviews</th>\n",
       "      <td>0.187879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMM - styler reviews</th>\n",
       "      <td>0.162963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      format error  polarity error-class  \\\n",
       "SMM - pot reviews         0.187879                   0.0   \n",
       "SMM - styler reviews      0.162963                   0.0   \n",
       "\n",
       "                      polarity format error  \n",
       "SMM - pot reviews                  0.000000  \n",
       "SMM - styler reviews               0.026549  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_triplets_pot, f_rate_pot, p_rate_pot, pf_rate_pot = load_ASTE_results(res_path_pot)\n",
    "res_triplets_styler, f_rate_styler, p_rate_styler, pf_rate_styler = load_ASTE_results(res_path_styler)\n",
    "\n",
    "model_name_short = model_name.split('/')[-1]\n",
    "print(f\"Model: {model_name_short}\")\n",
    "\n",
    "df_load = pd.DataFrame(columns=('format error', 'polarity error-class', 'polarity format error'))\n",
    "df_load.loc[1] = [f_rate_pot, p_rate_pot, pf_rate_pot]\n",
    "df_load.loc[2] = [f_rate_styler, p_rate_styler, pf_rate_styler]\n",
    "\n",
    "df_load.index = ['SMM - pot reviews',\n",
    "                'SMM - styler reviews']\n",
    "df_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b2cd966-7d84-4efe-abcc-1537e39e10c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: В качестве подарка лучше не придумать, именинница рада и очень благодарна\n",
      "\n",
      "Триплеты:\n",
      "подарок: не придумать: положительно\n",
      "подарок: именинка рада и очень благодарна: положительно\n"
     ]
    }
   ],
   "source": [
    "idx = 19\n",
    "\n",
    "print(texts_styler[idx])\n",
    "print(\"Триплеты:\")\n",
    "print_triplets(res_triplets_styler[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fa2ee9d-0fe2-48a6-ad7b-057e96315771",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_triplets_styler = []\n",
    "for triplets in res_triplets_styler:\n",
    "    all_triplets_styler.extend(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "839d0a5f-c096-40f9-a01b-41bafa549b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['насадка', 'для длинных и коротких', 'положительно']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_triplets_styler[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6223eba-9999-44ef-8492-44e61b676aae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
